services:
  nginx:
    build: 
      context: ./nginx
      dockerfile: Dockerfile
    ports:
      - "9000:80"
      - "8443:443"
    volumes:
      - ./allsides_next/backend/staticfiles:/app/backend/staticfiles
      - ./allsides_next/backend/mediafiles:/app/backend/mediafiles
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./certbot/conf:/etc/letsencrypt
      - ./certbot/www:/var/www/certbot
    depends_on:
      - backend
      - frontend
    networks:
      - app_network

  frontend:
    build:
      context: ./allsides_next/frontend
      dockerfile: Dockerfile
      args:
        - NODE_ENV=${NODE_ENV:-production}
    ports:
      - "3000:3000"
    env_file:
      - .env
    environment:
      - NEXT_PUBLIC_API_URL=http://your-ec2-public-ip:9000/api
      - NODE_ENV=${NODE_ENV:-production}
      - HOSTNAME=0.0.0.0
      - NEXT_PRIVATE_HOST=0.0.0.0
    depends_on:
      - backend
    networks:
      - app_network
    

  backend:
    build:
      context: ./allsides_next/backend
      dockerfile: Dockerfile
      args:
        - ENVIRONMENT=${NODE_ENV:-production}
        - OPENAI_API_KEY=${OPENAI_API_KEY}
        - TAVILY_API_KEY=${TAVILY_API_KEY}
        - GOOGLE_API_KEY=${GOOGLE_API_KEY}
        - LINKUP_API_KEY=${LINKUP_API_KEY}
        - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
        - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
        - LANGFUSE_HOST=${LANGFUSE_HOST}
    expose:
      - "8000"
    volumes:
      - ./allsides_next/backend:/app
      - ./allsides_next/backend/static:/app/static
      - ./allsides_next/backend/staticfiles:/app/staticfiles
      - ./allsides_next/backend/mediafiles:/app/mediafiles
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: >
      sh -c "python manage.py collectstatic --noinput &&
             python manage.py migrate &&
             gunicorn core.wsgi:application --bind 0.0.0.0:8000 --workers 4 --threads 4 --timeout 600"
    env_file:
      - .env
    environment:
      - DEBUG=${DEBUG:-False}
      - SECRET_KEY=${SECRET_KEY}
      - POSTGRES_DB=${AURORA_DB_NAME}
      - POSTGRES_USER=${AURORA_USER}
      - POSTGRES_PASSWORD=${AURORA_PASSWORD}
      - DATABASE_URL=postgres://${AURORA_USER}:${AURORA_PASSWORD}@${AURORA_ENDPOINT}:${AURORA_PORT}/${AURORA_DB_NAME}?sslmode=${AURORA_SSL_MODE}
      - ALLOWED_HOSTS=*
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - LINKUP_API_KEY=${LINKUP_API_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_HOST=${LANGFUSE_HOST}
      - REDIS_HOST=${REDIS_HOST:-redis}
      - OLLAMA_HOST=ollama:11434
      - OLLAMA_API_URL=http://ollama:11434/api
      - VLLM_API_URL=http://vllm:8000/v1
    depends_on:
      - redis
      - ollama
    networks:
      - app_network

  # Local Redis - keep if not using ElastiCache
  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --save 60 1 --save 300 100
    volumes:
      - redis_data:/data
    networks:
      - app_network

  ollama:
    image: ollama/ollama:latest
    container_name: allsides_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_NUM_THREAD=4
      - OLLAMA_FLASH_ATTENTION=true
      - OLLAMA_KEEP_ALIVE=600
      - OLLAMA_CONCURRENCY_LIMIT=2
      - OLLAMA_GPU=false
      - OLLAMA_CPU_COUNT=4
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - app_network
    # CPU-only deployment - optimized for 4-core system
    deploy:
      resources:
        limits:
          cpus: '3.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G

  vllm:
    image: vllm/vllm-openai:latest
    container_name: allsides_vllm
    ports:
      - "8001:8000"
    volumes:
      - vllm_cache:/root/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
      - VLLM_LOGGING_LEVEL=DEBUG
      - OMP_NUM_THREADS=2
      - CUDA_VISIBLE_DEVICES=""
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    entrypoint: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    command: >
      --model microsoft/DialoGPT-small
      --port 8000
      --host 0.0.0.0
      --trust-remote-code
      --dtype float32
      --max-model-len 512
      --served-model-name dialogpt-small
      --tensor-parallel-size 1
      --disable-log-stats
      --enforce-eager
      --max-num-seqs 2
      --block-size 8
      --device cpu
      --worker-use-ray
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s
    restart: unless-stopped
    networks:
      - app_network
    # CPU-only deployment - optimized for 4-core system
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 8G

volumes:
  static_volume:
  media_volume:
  redis_data:
  ollama_data:
  vllm_cache: 
  # Removed postgres_data and pgadmin_data volumes

networks:
  app_network:
    driver: bridge