# LangGraph Open Deep Research Integration Plan

## 1. Overview

This document outlines a plan to seamlessly integrate LangChain's Open Deep Research platform into our existing system. The integration will replace the current card-based approach with a powerful research engine that generates comprehensive long-form responses based on user queries.

## 2. Current System Analysis

Our current platform appears to be a Django-based web application with:
- Frontend: Next.js application (in `allsides_next/frontend/`)
- Backend: Django REST API (in `allsides_next/backend/`)
- Current search implementation: Card-based system with structured responses
- Current AI integration: Main AI processing in `allsides_next/backend/api/main_v3.py` with Redis caching

## 3. Open Deep Research Overview

LangChain's Open Deep Research is an open-source assistant that:
- Automates research on any topic
- Produces customizable long-form reports
- Uses a two-phase approach:
  1. **Plan and Execute**: Planning the research with human-in-the-loop approval
  2. **Research and Write**: Conducting deep research and writing cohesive reports
  3. **Customizable Types**: Managing different report structures

## 4. Integration Architecture

### 4.1 Core Components
- **LangGraph Integration Layer**: New Python module to interface with Open Deep Research
- **API Endpoints**: New endpoints to handle research requests and streaming responses
- **Configuration Management**: System to manage and customize research parameters
- **Frontend Updates**: UI changes to support long-form research results

### 4.2 Data Flow
1. User submits research query through existing interface
2. Backend forwards request to Open Deep Research integration layer
3. Integration layer initializes and runs research graph
4. Results are streamed back to user as they become available
5. Final report is presented to user and optionally cached for future reference

## 5. Implementation Plan

### 5.1 Backend Implementation

#### 5.1.1 Dependencies
Add to requirements.txt:
```
open-deep-research
langgraph
langchain
langchain-openai  # If using OpenAI models
langchain-anthropic  # If using Anthropic models
```

#### 5.1.2 Integration Module Structure
Create a new module in `allsides_next/backend/api/research/`:
- `__init__.py`: Module initialization
- `graph_manager.py`: Interface with Open Deep Research graph
- `models.py`: Data models for research config and results
- `serializers.py`: Serializers for API communication
- `utils.py`: Utility functions

#### 5.1.3 Research Manager Class
```python
# Pseudocode for graph_manager.py
from open_deep_research.graph import builder
from langgraph.checkpoint.memory import MemorySaver
import uuid

class ResearchManager:
    def __init__(self, search_api="tavily", planner_model="claude-3-7-sonnet-latest", 
                 writer_model="claude-3-5-sonnet-latest", max_search_depth=2):
        self.memory = MemorySaver()
        self.graph = builder.compile(checkpointer=self.memory)
        self.config = {
            "search_api": search_api,
            "planner_provider": "anthropic",  # Configurable
            "planner_model": planner_model,
            "writer_provider": "anthropic",   # Configurable
            "writer_model": writer_model,
            "max_search_depth": max_search_depth,
        }
    
    async def start_research(self, topic):
        """Initialize research on a topic"""
        thread = {
            "configurable": {
                "thread_id": str(uuid.uuid4()),
                **self.config
            }
        }
        return self.graph, thread, {"topic": topic}
    
    async def stream_results(self, graph, thread, inputs):
        """Stream research results as they become available"""
        async for event in graph.astream(inputs, thread, stream_mode="updates"):
            yield event
```

#### 5.1.4 API Views
Extend current views.py to add research endpoints:
```python
# Pseudocode for views integration
class ResearchView(APIView):
    async def post(self, request):
        """Initiate a research request"""
        topic = request.data.get("query")
        manager = ResearchManager()
        graph, thread, inputs = await manager.start_research(topic)
        
        # Store session info for streaming
        request.session["research"] = {
            "thread_id": thread["configurable"]["thread_id"],
            "status": "planning"
        }
        
        # Return initial response with session ID
        return Response({"thread_id": thread["configurable"]["thread_id"]})

class ResearchStreamView(APIView):
    async def get(self, request, thread_id):
        """Stream research results"""
        # Set up streaming response
        async def event_stream():
            manager = ResearchManager()
            # Retrieve thread and graph state
            # Stream results
            async for event in manager.stream_results(graph, thread, inputs):
                yield f"data: {json.dumps(event)}\n\n"
                
        return StreamingHttpResponse(event_stream(), content_type='text/event-stream')
```

### 5.2 Frontend Implementation

The frontend would need updates to:
1. Display a research UI with topic input
2. Handle streaming responses with appropriate loading states
3. Format and present the long-form research content
4. Potentially offer controls for refining research

### 5.3 Configuration Management

Create a configuration system to customize:
- Search APIs (Tavily, Perplexity, Exa, ArXiv, PubMed, etc.)
- LLM models for planning and writing
- Report structure templates
- Research depth parameters

## 6. API Keys and Environment Setup

Required environment variables:
```
# LLM API Keys
ANTHROPIC_API_KEY=<key>  # If using Anthropic
OPENAI_API_KEY=<key>     # If using OpenAI

# Search API Keys
TAVILY_API_KEY=<key>     # If using Tavily
PERPLEXITY_API_KEY=<key> # If using Perplexity
EXA_API_KEY=<key>        # If using Exa
# Additional API keys as needed
```

## 7. Customization Capabilities

### 7.1 Prompt Customization
The Open Deep Research platform allows customization of prompts for both planning and writing phases. We can extend this by:

1. Creating a prompt management system in our admin interface
2. Allowing admin users to modify prompts for different research types
3. Storing custom prompts in the database and injecting them into the research configuration

### 7.2 Report Structure Customization
We can define custom report structures for different use cases:
```python
# Example custom report structures
REPORT_STRUCTURES = {
    "default": {
        "sections": ["Introduction", "Background", "Analysis", "Conclusion"]
    },
    "academic": {
        "sections": ["Abstract", "Introduction", "Literature Review", "Methodology", "Results", "Discussion", "Conclusion"]
    },
    "market_analysis": {
        "sections": ["Executive Summary", "Market Overview", "Competitive Landscape", "SWOT Analysis", "Future Outlook"]
    }
}
```

## 8. Error Handling and Fallbacks

Implement robust error handling:
1. API key validation and fallback search methods
2. Timeout handling for long-running research
3. Graceful degradation if specific services are unavailable
4. Caching of partial results to resume interrupted research

## 9. Testing Strategy

- Unit tests for the integration layer
- Integration tests with mock LLM and search responses
- End-to-end tests of the complete research flow
- Performance testing with various query complexities

## 10. Deployment and Scaling Considerations

- Implement async processing for handling multiple concurrent research requests
- Optimize memory usage for long-running research sessions
- Consider serverless deployment for scaling research workers independently
- Implement result caching for popular topics

## 11. Monitoring and Analytics

- Track research session metrics (duration, tokens used, search queries)
- Monitor API costs across different providers
- Analyze user satisfaction with research results
- Identify common failure patterns for improvement

## 12. Future Enhancements

- Interactive research refinement during the planning phase
- User feedback incorporation into research quality
- Multi-modal research capabilities (images, charts, data visualization)
- Research citation and source tracking

## 13. Migration Strategy

Phase 1: Parallel implementation with feature flag
Phase 2: Beta testing with select users
Phase 3: Gradual rollout with option to use classic mode
Phase 4: Complete transition to new research system 